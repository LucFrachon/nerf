{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dda2469-4329-45c4-aa1e-cf015527129e",
   "metadata": {},
   "source": [
    "# Neural Radiance Fields\n",
    "---\n",
    "Luc Frachon\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45313ba1-c167-4d05-96ba-f865693fcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as td\n",
    "import torchvision as tv\n",
    "import PIL\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274ca05-169e-44da-92b5-a14d712908c5",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5938ed-233b-4e79-a97f-3eed00a8c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "DATA_CONFIG = OrderedDict(\n",
    "    np_data_filepath = \"./data/tiny_nerf_data.npz\", \n",
    "    height = 100,\n",
    "    width = 100,\n",
    "    n_samples_per_ray = 64,\n",
    "    near_bound = 1.,\n",
    "    far_bound = 6.,\n",
    "    fine_sample_multiplicator = 2.,\n",
    "    test_img_idx = 73,\n",
    "    random_state = 0,\n",
    "    precision = torch.float16,\n",
    ")\n",
    "MODULE_CONFIG = OrderedDict(\n",
    "    coord_dims = 10,\n",
    "    direction_dims = 4,\n",
    "    n_layers = 8,\n",
    "    skip_at_layer = 5,\n",
    "    layer_dim = 128,\n",
    "    use_fine_samples = True,\n",
    "    num_workers = 0,\n",
    "    learning_rate = 1e-4,\n",
    "    batch_size = 4096,\n",
    ")\n",
    "RUN_CONFIG = OrderedDict(\n",
    "    logging = True,\n",
    "    run_name = \"MiniNeRF_fine_float16_model_v2\",\n",
    "    run_notes = \"Half-precision, normal sampling density, fine multiplicator==2\",\n",
    "    early_stopping = True,\n",
    "    early_stop_delta = 1e-4,\n",
    "    early_stop_patience = 10000,  # iterations, not epochs\n",
    "    max_epochs = 50,\n",
    "    gradient_clip_val = 2.,\n",
    "    log_every_n_steps = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167dd40b-029a-4c7d-ba68-19b9e06f6b0a",
   "metadata": {},
   "source": [
    "## Some convenience functions for plotting in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f7de7-ef48-4717-a79d-85bb0e94b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_unflatten(values: torch.Tensor, height: int, width: int):\n",
    "    return values.reshape(height, width, -1, values.shape[2]).detach().cpu().numpy()\n",
    "\n",
    "def bin_dims(values: np.ndarray, bin_sz_x: int, bin_sz_y: int, bin_sz_z: int):\n",
    "    sz_x, sz_y, sz_z = values.shape[-3], values.shape[-2], values.shape[-1]\n",
    "    return np.mean(\n",
    "        values.reshape(\n",
    "            -1,\n",
    "            sz_x//bin_sz_x, \n",
    "            bin_sz_x,\n",
    "            sz_y//bin_sz_y,\n",
    "            bin_sz_y,\n",
    "            sz_z//bin_sz_z,\n",
    "            bin_sz_z,\n",
    "        ),\n",
    "        axis=(-5, -3, -1)\n",
    "    ).squeeze()\n",
    "\n",
    "def scatter3d(values, fig_kwargs={}, scatter_kwargs={}):\n",
    "    # Make a 3D plot of densities\n",
    "    fig = plt.figure(**fig_kwargs)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x, y, z = np.meshgrid(range(values.shape[-3]), range(values.shape[-2]), range(values.shape[-1]))\n",
    "\n",
    "    # Define the color of each voxel based on the tensor values\n",
    "    colors = np.ravel(values)\n",
    "    # Create the voxel plot\n",
    "    ax.scatter(x, y, z, c=colors, cmap='coolwarm', **scatter_kwargs)\n",
    "    # Add axis labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.show();\n",
    "    \n",
    "def quiver3d(values, fig_kwargs={}, arrow_kwargs={}):\n",
    "    fig = plt.figure(**fig_kwargs)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    xs, ys, zs = values[0], values[1], values[2]\n",
    "    us, vs, ws = values[3], values[4], values[5]\n",
    "    ax.quiver(xs, ys, zs, us, vs, ws, **arrow_kwargs)\n",
    "    # Add axis labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fcd941-1744-4cb9-bf67-62ec657c4d83",
   "metadata": {},
   "source": [
    "## Extracting rays from the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fe756-1a28-46d1-b096-8418d4890fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays_from_image(width: int, height: int, focal: float, pose_matrix: torch.Tensor):\n",
    "    \"\"\"\n",
    "    The rays are oriented from the scene towards the camera. The coordinates of the\n",
    "    ray's unit vectors use x towards the RHS of the image, y towards the top, \n",
    "    and z towards the focal point.\n",
    "   \n",
    "   pose_matrix: A (n_image, 4, 4) tensor containing the pose angles and translation vector for the\n",
    "      camera position for all images.\n",
    "   \n",
    "   :return: \n",
    "    origin_world_coords: The camera position (all rays' origin) as a 3D vector expressed in the \n",
    "    world coordinate system, expanded into a (height, width, 3) tensor.\n",
    "    dir_world_coors: The directions of each ray (pixel), as unit vectors, in a tensor of\n",
    "    dimensions (height, width, 3).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create 2 meshgrids with coordinates centred around the image's centre.\n",
    "    if isinstance(focal, np.ndarray):\n",
    "        focal = focal.item()\n",
    "    \n",
    "    rotation = pose_matrix[..., :3, :3]\n",
    "    camera_pos = pose_matrix[..., :3, -1]\n",
    "\n",
    "    if not isinstance(rotation, torch.Tensor):\n",
    "        rotation = torch.tensor(rotation, dtype=torch.float32)\n",
    "    if not isinstance(camera_pos, torch.Tensor):\n",
    "        camera_pos = torch.tensor(camera_pos, dtype=torch.float32)\n",
    "        \n",
    "    n_img = rotation.size(0)\n",
    "    xs, ys = torch.meshgrid(\n",
    "        torch.arange(width, dtype=torch.float32), \n",
    "        torch.arange(height, dtype=torch.float32),\n",
    "        indexing='xy'\n",
    "    )\n",
    "    xs, ys = xs - width / 2., ys - height / 2.  # centre around 0\n",
    " \n",
    "    dir_cam_coords = torch.stack([\n",
    "        xs, \n",
    "        -ys,  # in the meshgrid, y points toward the bottom of the picture\n",
    "        -torch.zeros_like(xs).fill_(focal)\n",
    "    ], dim=-1)\n",
    "    dir_cam_coords /= focal\n",
    "    \n",
    "    dir_cam_coords = dir_cam_coords.reshape((-1, 3))  # reshape before the matmul\n",
    "    dir_world_coords = torch.matmul(\n",
    "        dir_cam_coords,\n",
    "        rotation.transpose(-1, -2),\n",
    "    ).to(DATA_CONFIG['precision']).reshape((n_img * height * width, 1, 3))\n",
    "        \n",
    "    origins = camera_pos.to(\n",
    "        DATA_CONFIG['precision']\n",
    "    ).unsqueeze(-1).transpose(-1, -2)\n",
    "    origins = origins.repeat_interleave(\n",
    "        height * width,\n",
    "        dim=0,\n",
    "        output_size=n_img * height * width\n",
    "    )\n",
    "    \n",
    "    return origins, dir_world_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee62c67-89e4-4593-8645-1d394f9dc2fe",
   "metadata": {},
   "source": [
    "The plot below shows the position and ray directions for all images in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835642f2-af20-499b-b1b7-c07d92427c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "i = 73\n",
    "data = np.load(DATA_CONFIG['np_data_filepath'])\n",
    "%timeit origins, dirs = get_rays_from_image(100, 100, data['focal'].item(), data['poses'])\n",
    "# orig_resh = origins.reshape(106, 100, 100, -1).numpy()\n",
    "# dirs_resh = dirs.reshape(106, 100, 100, -1).numpy()\n",
    "# bin_sz_x, bin_sz_y = 20, 20\n",
    "# bin_sz_z = 10\n",
    "        \n",
    "# dirs_binned = np.mean(dirs_resh.reshape(106, 100//bin_sz_y, bin_sz_y, 100//bin_sz_x, bin_sz_x, dirs_resh.shape[-1]), axis=(2,4))\n",
    "# orig_binned = np.mean(orig_resh.reshape(106, 100//bin_sz_y, bin_sz_y, 100//bin_sz_x, bin_sz_x, orig_resh.shape[-1]), axis=(2,4))\n",
    "# dirs_binned = dirs_resh\n",
    "# orig_binned = orig_resh\n",
    "# fig = plt.figure(figsize=(20, 8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# x, y, z = orig_binned[ ..., 0], orig_binned[ ..., 1], orig_binned[ ..., 2]\n",
    "# u, v, w = dirs_binned[ ..., 0], dirs_binned[ ..., 1], dirs_binned[ ..., 2]\n",
    "\n",
    "# ax.quiver(x, y, z, u, v, w, normalize=False, length=0.5)\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b662a676-4585-4332-958a-4895e4899212",
   "metadata": {},
   "source": [
    "## Sampling points along rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ce59e-192f-484c-a02a-3ac516983bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_samples_from_rays(\n",
    "    origins, directions, near_bound, far_bound, n_coarse, noisy = True, distribution = None, fine_sample_multiplicator = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a batch of ray origins and directions, 2 behaviours are possible: \n",
    "    - If distribution is None: Divide the distance between far_bound and near_bound by n_samples to \n",
    "    create n_samples intervals, then sample uniformly at random from each interval.\n",
    "    - If distribution is not None but is passed as a PDF, sample `n_samples * fine_sample_multiplicator` \n",
    "    points from this distribution. \n",
    "    \n",
    "    `origins`: Tensor of shape (n_img * h * w, 1, 3). Coordinates of the origin of each ray in the batch (focal \n",
    "        point of their respective camera). Note that during inference, all the rays are from the same image,\n",
    "        therefore  the (x, y, z) coordinates are the same for all rays.\n",
    "    `directions`: Tensor of shape (n_img * h * w, 1, 3). Coordinates of the unit vector that defines each ray in the batch.\n",
    "    `distribution`: When performing fine sampling, this is the distribution to use. If None, uses the noisy uniform distribution \n",
    "        by bin.\n",
    "    `fine_sample_multiplicator`: Set to None to only perform coarse sampling.\n",
    "        \n",
    "    :return: \n",
    "    sample_coords: The coordinates and directions of each sampled point, as a (n_img * h * w, n_samples, 6) tensor.\n",
    "    sample_distances: The distance values of each sampled point along its ray, as a (n_img * h * w, n_samples, 1) tensor.\n",
    "    \"\"\"\n",
    "    bin_centres_c = torch.linspace(near_bound, far_bound, steps=n_coarse).to(origins.device)\n",
    "    if noisy:\n",
    "        noise = torch.rand((origins.shape[0], n_coarse, 1), dtype=origins.dtype).to(origins.device)\n",
    "        noise *= (far_bound - near_bound) / n_coarse\n",
    "    else:\n",
    "        noise = torch.zeros((origins.shape[0], n_coarse, 1), dtype=origins.dtype).to(origins.device)\n",
    "        \n",
    "    coarse_distances = bin_centres_c[None, :, None] + noise\n",
    "    # Ensure we remain within the bounds. .clamp is not implemented for float16 so we do it manually :-)\n",
    "    coarse_distances = torch.min(\n",
    "        torch.max(coarse_distances, torch.zeros_like(coarse_distances).fill_(near_bound)), \n",
    "        torch.zeros_like(coarse_distances).fill_(far_bound)\n",
    "    )\n",
    "    \n",
    "    if fine_sample_multiplicator is not None:\n",
    "        n_fine = int(n_coarse * fine_sample_multiplicator)\n",
    "        if distribution is not None:\n",
    "            fine_distances = distribution.sample(n_fine).to(origins.device)\n",
    "        else:\n",
    "            bin_centres_f = torch.linspace(near_bound, far_bound, steps=n_fine).to(origins.device)\n",
    "            noise = torch.rand((origins.shape[0], n_fine, 1), dtype=origins.dtype).to(origins.device)\n",
    "            noise *= (far_bound - near_bound) / n_fine\n",
    "            fine_distances = bin_centres_f[None, :, None] + noise\n",
    "            # Ensure we remain within the bounds. .clamp is not implemented for float16 so we do it manually :-)\n",
    "            fine_distances = torch.min(\n",
    "                torch.max(fine_distances, torch.zeros_like(fine_distances).fill_(near_bound)),\n",
    "                torch.zeros_like(fine_distances).fill_(far_bound)\n",
    "            )\n",
    "\n",
    "        n_samples = n_fine + n_coarse\n",
    "        sample_distances = torch.concat([coarse_distances, fine_distances], dim=1)\n",
    "    else:\n",
    "        n_samples = n_coarse\n",
    "        sample_distances = coarse_distances\n",
    "\n",
    "    samples_distances = torch.sort(sample_distances, dim=1)\n",
    "    sample_coords = origins + directions * sample_distances\n",
    "    # Add coordinates of the unit vector for corresponding ray:\n",
    "    directions = directions.expand(-1, n_samples, -1)\n",
    "    sample_coords = torch.concat([sample_coords, directions], dim=-1)\n",
    "    \n",
    "    return sample_coords, sample_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa652dc5-6696-4722-bfa8-db97e9514aac",
   "metadata": {},
   "source": [
    "This plot shows the rays corresponding to the first 10,000 pixels, i.e. the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58b7eb-5894-46f8-9c1e-8dcedf1fabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# origins, directions = get_rays_from_image(100, 100, data['focal'].item(), data['poses'])\n",
    "# coords_all, dist_all = get_samples_from_rays(\n",
    "#     origins, directions, DATA_CONFIG['near_bound'], DATA_CONFIG['far_bound'], 32, fine_sample_multiplicator=2\n",
    "# )\n",
    "# coord_all = coords_all.cpu()\n",
    "# coords_np = to_numpy_unflatten(coords_all[0:10000], 100, 100)\n",
    "# coords_np = bin_dims(coords_np, 1, 1, 1)\n",
    "# coords_np.shape\n",
    "# quiver3d(\n",
    "#     coords_np,\n",
    "#     fig_kwargs={'figsize': (10, 9)},\n",
    "#     arrow_kwargs={\n",
    "#         'length': 0.15,\n",
    "#         'normalize': True,\n",
    "#         'arrow_length_ratio': 0.1\n",
    "#     }\n",
    "# )\n",
    "# plt.xlim(-5, 5)\n",
    "# plt.ylim(-5, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898b953-1a05-4037-9790-a613b791d1be",
   "metadata": {},
   "source": [
    "## Implement the rendering equation\n",
    "(see blog post for explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b846997-7af7-43ea-96e9-65309d02cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_image_colours(sample_distances, densities, colours):\n",
    "    \"\"\"\n",
    "    Uses the rendering formula to compute the resulting colour at every position of the image.\n",
    "    \n",
    "    sample_coordinates: (BS, n_samples, 6), where n_samples is the number of sampling points along\n",
    "        each ray.\n",
    "    sample_distances: (BS, n_samples, 1)\n",
    "    densities: (BS, n_samples, 1) Density tensor. Contains one value for each sampling point of each \n",
    "        ray in the image. These values can be provided by the NeRF model and are always positive.\n",
    "    colours: (BS, n_samples, 3) Colour tensor. Contains 3 values (R, G, B) for each sampling point\n",
    "        of each ray in the image. These values can be provided by the NeRF model and are bounded \n",
    "        between 0 and 1.\n",
    "        \n",
    "    :return:\n",
    "    colours: (BS, 3) Rendered (R, G, B) values for each pixel\n",
    "    weights: (BS, n_samples, 1) The factor associated with that point of the ray, \n",
    "        used to weight the emitted colour at that point in that direction. Can be re-used to\n",
    "        perform fine sampling.\n",
    "    \"\"\"\n",
    "    last_pos_distance = 1e-10 + torch.zeros_like(sample_distances[..., [-1], :])\n",
    "    distance_to_next = sample_distances[..., 1:, :] - sample_distances[..., :-1, :]\n",
    "    distance_to_next = torch.concat([distance_to_next, last_pos_distance], dim=-2)\n",
    "    sigma_delta = distance_to_next * densities\n",
    "    # Offset sigma_delta by 1 to compute the cum. sum. because the sum at index i includes\n",
    "    # points from 1 to i-1.\n",
    "    sigma_delta_shifted = torch.concat(\n",
    "        [1e-10 + torch.zeros_like(sigma_delta[..., [0], :]), sigma_delta[..., :-1, :]], \n",
    "        dim=-2\n",
    "    )\n",
    "    cum_transmittance = torch.exp(-torch.cumsum(sigma_delta_shifted, dim=-2))\n",
    "    weights = cum_transmittance * (1 - torch.exp(-sigma_delta)) + 1e-10\n",
    "    rendered_colours = torch.sum(weights * colours, dim=-2)\n",
    "\n",
    "    return rendered_colours, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e356b3-776a-462a-9cb8-d1f256852a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_dims(x, shift):\n",
    "    \"\"\"\n",
    "    Like permute, changes the sequence of dimensions in a tensor. Unlike permute, it can only\n",
    "    shift dimensions cyclically towards the right if `shift` > 0 or the left if `shift` < 0.\n",
    "    The other difference (and the benefit) is that where permute can only handle a fixed number \n",
    "    of predefined dimensions, this function works with any number.\n",
    "    \"\"\"\n",
    "    dims = torch.arange(x.dim())\n",
    "    new_dims = dims.roll(shifts=shift)\n",
    "    return x.permute(*new_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe1abf-77f1-4886-87b9-5f9ae0761056",
   "metadata": {},
   "source": [
    "## Distribution used for fine sampling\n",
    "Not fully tested yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c75079-ac82-44c4-a128-87cf8f332af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseConstant(td.Distribution):\n",
    "    def __init__(self, pdf, bin_centres, far_bound, validate_args=None):\n",
    "        self._pdf = pdf  .to(torch.float32)\n",
    "        self._support = bin_centres.to(torch.float32)\n",
    "        self.intervals = torch.concat(\n",
    "            [bin_centres[..., 1:, :], torch.zeros_like(bin_centres[..., [0], :]).fill_(far_bound)],\n",
    "            dim=1,\n",
    "        ) - bin_centres\n",
    "        self._cdf = torch.cumsum(pdf, dim=-2)\n",
    "        super().__init__(torch.empty(0), validate_args=validate_args)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, n_samples = 1):\n",
    "        # Generate samples from a uniform distribution:\n",
    "        uniform = td.Uniform(self._support[..., 0, :], self._support[..., -1, :])\n",
    "        uni_samples = uniform.sample((n_samples,))\n",
    "        \n",
    "        # Map the samples to the piecewise constant distribution. Since searchsorted operates only on the last dimension,\n",
    "        # we need to perform some permutations.\n",
    "        cdf_indices = torch.searchsorted(\n",
    "            self._cdf.transpose(-1, -2).contiguous(),\n",
    "            shift_dims(uniform.cdf(uni_samples), -1).contiguous(),\n",
    "            side='right'\n",
    "        )\n",
    "        cdf_indices = cdf_indices.transpose(-1, -2).clamp(0, self._support.size(-2) - 1)\n",
    "\n",
    "        # From cdf_samples, we derive the sampling frequency within each constant piece of the PDF.\n",
    "        # Within each piece, we sample uniformly\n",
    "        noise = torch.rand_like(shift_dims(uni_samples, -1).transpose(-1, -2))\n",
    "        final_samples = torch.gather(self._support, index=cdf_indices, dim=-2) \n",
    "        # final_samples, _ = torch.sort(final_samples, dim=-2)\n",
    "        final_samples += noise * torch.gather(self.intervals, index=cdf_indices, dim=-2)\n",
    "        return final_samples.to(DATA_CONFIG['precision'])\n",
    "        \n",
    "    @property\n",
    "    def support(self):\n",
    "        return self._support\n",
    "    \n",
    "    @property\n",
    "    def pdf(self):\n",
    "        return self._pdf\n",
    "\n",
    "    @property\n",
    "    def cdf(self):\n",
    "        return self._cdf\n",
    "    \n",
    "    def update_pdf(self, new_pdf):\n",
    "        self._pdf = new_pdf\n",
    "        self._cdf = torch.cumsum(pdf, dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c0cc3-50ac-45fc-965b-aa4a241ea28b",
   "metadata": {},
   "source": [
    "## Data Loading and Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345ef1e-b67b-4f2e-8e6c-167515f7830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayDataProvider:\n",
    "    def __init__(\n",
    "        self, \n",
    "        np_data_filepath = \"./data/tiny_nerf_data.npz\", \n",
    "        np_data = None,\n",
    "        n_samples_per_ray = 64,\n",
    "        near_bound = 2.,\n",
    "        far_bound = 6.,\n",
    "        initial_distribution = None,\n",
    "        fine_sample_multiplicator = None,\n",
    "        test_img_idx: int = None,\n",
    "        random_state = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        np_data_filepath: str; Path to a Numpy data file (.npz) containing the scene's images,\n",
    "            poses, focal length, and other metadata.\n",
    "        test_img_idx: int or None; Index of image and pose to use as a test example. Set to None\n",
    "            for a randomly chosen image.\n",
    "        \"\"\"\n",
    "        if np_data is None:\n",
    "            data = np.load(np_data_filepath)\n",
    "        else: \n",
    "            data = np_data\n",
    "            \n",
    "        test_img_idx = np.random.randint(0, len(data['images'])) if test_img_idx is None else test_img_idx\n",
    "        self.test_idx = test_img_idx\n",
    "               \n",
    "        self.h, self.w = data['images'].shape[1], data['images'].shape[2]\n",
    "\n",
    "        images = torch.tensor(data['images'], dtype=DATA_CONFIG['precision'])\n",
    "        # Once test_idx is set, we move the image to the end of the data to separate it from the training set\n",
    "        images = torch.concat(\n",
    "            [\n",
    "                images[: self.test_idx],\n",
    "                images[self.test_idx + 1 :],\n",
    "                images[[self.test_idx]]\n",
    "            ], dim=0\n",
    "        )\n",
    "        \n",
    "        self.pixels = images.reshape(len(images) * self.h * self.w, 3)  # (n_img * h * w, 3)\n",
    "\n",
    "        # Do the same thing with poses\n",
    "        poses = torch.tensor(data['poses'])\n",
    "        self.poses = torch.concat(\n",
    "            [\n",
    "                poses[: self.test_idx],\n",
    "                poses[self.test_idx + 1 :],\n",
    "                poses[[self.test_idx]]\n",
    "            ], dim=0\n",
    "        )\n",
    "            \n",
    "        self.focal = data['focal'].item()\n",
    "        self.n_samples_per_ray = n_samples_per_ray\n",
    "        self.near = near_bound\n",
    "        self.far = far_bound\n",
    "        self.fine_sample_multiplicator = fine_sample_multiplicator\n",
    "        self.point_coords, self.distances = self.update_ray_samples(\n",
    "            True, \n",
    "            initial_distribution, \n",
    "            fine_sample_multiplicator\n",
    "        )\n",
    "               \n",
    "    def update_ray_samples(self, noisy = True, distribution = None, fine_sample_multiplicator = None):\n",
    "        org, drc = get_rays_from_image(self.w, self.h, self.focal, self.poses)\n",
    "        coords, distances = get_samples_from_rays(\n",
    "            org, \n",
    "            drc, \n",
    "            self.near, \n",
    "            self.far, \n",
    "            self.n_samples_per_ray, \n",
    "            noisy=noisy, \n",
    "            distribution=distribution, \n",
    "            fine_sample_multiplicator=fine_sample_multiplicator,\n",
    "        )         \n",
    "        self.point_coords = coords \n",
    "        self.distances = distances\n",
    "        \n",
    "        return self.point_coords, self.distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1e692-b816-4b77-8283-e92075edce21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RayDataset(Dataset):\n",
    "    def __init__(self, data_provider, train = True):\n",
    "        super().__init__()\n",
    "        self.data_provider = data_provider\n",
    "        self.num_pixels_per_img = data_provider.h * data_provider.w\n",
    "        self.pixels = data_provider.pixels\n",
    "        self.poses = data_provider.poses\n",
    "        self.point_coords, self.distances = data_provider.point_coords, data_provider.distances\n",
    "        self.train = train\n",
    "\n",
    "    def update_ray_samples(self, noisy = True, distribution = None, fine_sample_multiplicator = None):\n",
    "        \"\"\"\n",
    "        This can be exectued after each epoch to update the sampling distribution in the case\n",
    "        of fine sampling.\n",
    "        \"\"\"\n",
    "        self.data_provider.update_ray_samples(noisy, distribution, fine_sample_multiplicator)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            # Returns the total number of pixels in the training set\n",
    "            return len(self.pixels) - self.num_pixels_per_img\n",
    "        else:\n",
    "            # In inference, we always want 10,000 rays making up the single test picture.\n",
    "            return 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.train:\n",
    "            # Returned items have dim (n_samples_per_ray, 6), (n_samples_per_ray, 3), (1, 3), (1,)\n",
    "            return (\n",
    "                self.point_coords[idx], \n",
    "                self.distances[idx],\n",
    "                self.pixels[idx],\n",
    "                idx\n",
    "            )\n",
    "        else:\n",
    "            # Returned items have dim (n_samples_per_ray, 6), (n_samples_per_ray, 1), (h*w, 1, 3)\n",
    "            # They will gain a batch dim from the DataLoader, therefore use with batch size == 1 and\n",
    "            # squeeze(0) before sending to the model.\n",
    "            return (\n",
    "                self.point_coords[-self.num_pixels_per_img :],\n",
    "                self.distances[-self.num_pixels_per_img :],\n",
    "                self.pixels[-self.num_pixels_per_img :],  # there's only one image in the test set\n",
    "                idx\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5119d84-a8e0-49b2-8b91-2484274bff6f",
   "metadata": {},
   "source": [
    "## Neural model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3d357-4b84-495d-9e91-e0c38cf44445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, L_dim, n_coords, normalize = False):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"factors\", 2**(torch.arange(0, L_dim, dtype=torch.float32)) * torch.pi)\n",
    "        self.normalize = normalize\n",
    "        self.L_dim = L_dim\n",
    "        self.n_coords = n_coords\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        if self.normalize:\n",
    "            x_min = x.amin(dim=(-1, -2), keepdim=True)\n",
    "            x_max = x.amax(dim=(-1, -2), keepdim=True)\n",
    "            x = 2 * (x - x_max) / (x_max - x_min + 1e-20) + 1\n",
    "            \n",
    "        embedding = torch.zeros((x.size(0), x.size(-2), 2 * self.n_coords * self.L_dim)).to(x)\n",
    "        \n",
    "        for coord in range(self.n_coords):\n",
    "            embedding[... , coord * 2 * self.L_dim : (coord + 1) * 2 * self.L_dim - 1 : 2] = torch.sin(\n",
    "                x[... , [coord]] * self.factors\n",
    "            )\n",
    "            embedding[... , coord * 2 * self.L_dim + 1 : (coord + 1) * 2 * self.L_dim : 2] = torch.cos(\n",
    "                x[... , [coord]] * self.factors\n",
    "            )\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a5996-fe8f-4b81-981c-0410a34c9abd",
   "metadata": {},
   "source": [
    "Here is an example of positional encoding using a single coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533af729-feae-42ea-a7c6-26a313d470ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.linspace(-1., 1., steps=16).reshape(1, -1, 1)\n",
    "enc = PositionalEncoding(3, 1)\n",
    "pos_enc = enc(pos).squeeze().numpy()\n",
    "factors = enc.factors.repeat_interleave(2)\n",
    "columns = ['sin ', 'cos '] * 8\n",
    "columns = ['position'] + [c + f\"{f:.2f}x\" for c, f in zip(columns, factors.tolist())]\n",
    "print(columns)\n",
    "pos_enc = pd.DataFrame(np.concatenate([pos.squeeze(0).numpy(), pos_enc], axis=1), columns=columns)\n",
    "pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35086b6f-b19d-4b97-bbaa-4b887f529b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# plt.close()\n",
    "# pos_enc.plot(x='position', figsize=(8, 8))\n",
    "# plt.axvline(x=-0.6, c='black', linestyle='--', lw=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfa5e1-fa84-4cce-a850-4c8501f7f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation = nn.ReLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(dim_in, dim_out)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.layer(x)\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad054d2c-5d03-42cf-a404-2235b4b6f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeRF(nn.Module):\n",
    "    def __init__(self, coord_dims, direction_dims, n_layers, skip_at_layer, layer_dim = 256):\n",
    "        super().__init__()\n",
    "        self.layer_dim = layer_dim\n",
    "        self.coord_dims = coord_dims\n",
    "        self.coord_encoding = PositionalEncoding(coord_dims, 3, normalize=True)\n",
    "        self.direction_dims = direction_dims\n",
    "        self.dir_encoding = PositionalEncoding(direction_dims, 3, normalize=False)\n",
    "        self.input_layer = FCLayer(6 * coord_dims, layer_dim)\n",
    "        \n",
    "        self.layers_pre_skip = nn.Sequential(*((FCLayer(layer_dim, layer_dim),) * (skip_at_layer - 1)))\n",
    "        self.skip_layer = FCLayer(layer_dim + 6 * coord_dims, layer_dim)\n",
    "        self.layers_post_skip = nn.Sequential(\n",
    "            *((FCLayer(layer_dim, layer_dim),) * (n_layers - skip_at_layer - 3))\n",
    "        )\n",
    "        self.density_output = FCLayer(layer_dim, layer_dim + 1, activation=None)\n",
    "        self.direction_fc = FCLayer(layer_dim + 6 * direction_dims, layer_dim // 2)\n",
    "        self.rgb_output = FCLayer(layer_dim // 2, 3, activation=nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            x = x.squeeze(0)\n",
    "        \n",
    "        # x is (batch_size, n_points_per_ray, n_coords + n_directions = 6)\n",
    "        coords, dirs = x.split(x.size(-1) // 2, dim=-1)\n",
    "        assert coords.shape == dirs.shape == (x.size(0), x.size(1), x.size(2) // 2)\n",
    "        B, N_pts, N_coords, N_dirs = coords.size(0), coords.size(-2), coords.size(-1), dirs.size(-1)        \n",
    "        \n",
    "        coords = self.coord_encoding(coords)\n",
    "        assert coords.shape == (B, N_pts, 2 * self.coord_dims * N_coords)\n",
    "        \n",
    "        dirs = self.dir_encoding(dirs)\n",
    "        assert dirs.shape == (B, N_pts, 2 * self.direction_dims * N_dirs)\n",
    "        \n",
    "        h = self.input_layer(coords); assert h.shape == (B, N_pts, self.layer_dim)\n",
    "        h = self.layers_pre_skip(h); assert h.shape == (B, N_pts, self.layer_dim)\n",
    "        \n",
    "        h = torch.concat([h, coords], dim=-1)\n",
    "        assert h.shape == (B, N_pts, self.layer_dim + 2 * self.coord_dims * N_coords)\n",
    "        h = self.skip_layer(h)\n",
    "        h = self.layers_post_skip(h)\n",
    "        h = self.density_output(h); assert h.shape == (B, N_pts, self.layer_dim + 1)\n",
    "        \n",
    "        density, h = h.split([1, self.layer_dim], dim=-1)\n",
    "        \n",
    "        h = torch.concat([h, dirs], dim=-1)\n",
    "        assert h.shape == (B, N_pts, self.layer_dim + 2 * self.direction_dims * N_dirs)\n",
    "        \n",
    "        h = self.direction_fc(h)\n",
    "\n",
    "        return torch.relu(density), self.rgb_output(h)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed15987-66e0-44cd-a1f8-8c40b0315414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeRFModule(pl.LightningModule):\n",
    "    def __init__(self, batch_size = 512, learning_rate = 3e-4, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.coarse_model = SimpleNeRF(\n",
    "            coord_dims=self.hparams.coord_dims,\n",
    "            direction_dims=self.hparams.direction_dims,\n",
    "            n_layers=self.hparams.n_layers,\n",
    "            skip_at_layer=self.hparams.skip_at_layer,\n",
    "            layer_dim=self.hparams.layer_dim,\n",
    "        )\n",
    "        if self.hparams.use_fine_samples:\n",
    "            self.fine_model = SimpleNeRF(\n",
    "                coord_dims=self.hparams.coord_dims,\n",
    "                direction_dims=self.hparams.direction_dims,\n",
    "                n_layers=self.hparams.n_layers,\n",
    "                skip_at_layer=self.hparams.skip_at_layer,\n",
    "                layer_dim=self.hparams.layer_dim,\n",
    "            )\n",
    "            self.weights = None\n",
    "            self.centres = None\n",
    "        self.log_to_wanb = self.hparams.logging\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hparams.use_fine_samples:\n",
    "            return self.coarse_model(x), self.fine_model(x)\n",
    "        else:\n",
    "            return self.coarse_model(x)\n",
    "    \n",
    "    def loss_function(self, img_colours_coarse, y_c, img_colours_fine = None, y_f=None):\n",
    "        if self.hparams.use_fine_samples:\n",
    "            return F.mse_loss(img_colours_coarse, y_c).mean() + F.mse_loss(img_colours_fine, y_f).mean()\n",
    "        else:\n",
    "            return F.mse_loss(img_colours_coarse, y_c).mean()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        points_c, distances_c, y_c, idxs_c = batch[\"coarse\"]\n",
    "        density_c, rgb_c = self.coarse_model(points_c)\n",
    "        img_cols_c, weights = render_image_colours(distances_c, density_c, rgb_c)\n",
    "\n",
    "        if self.hparams.use_fine_samples:\n",
    "            points_f, distances_f, y_f, _ = batch[\"fine\"]\n",
    "            density_f, rgb_f = self.fine_model(points_f)\n",
    "            img_cols_f, _ = render_image_colours(distances_f, density_f, rgb_f)\n",
    "            loss = self.loss_function(img_cols_c, y_c, img_cols_f, y_f)\n",
    "        else:\n",
    "            loss = self.loss_function(img_cols_c, y_c)\n",
    "            \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=self.log_to_wanb)\n",
    "        \n",
    "        # We need to return the indexes of the sample rays seen in this step to know which positions\n",
    "        # of the weights tensor to update when creating the new fine distribution's PDF.\n",
    "        return {\"loss\": loss, \"weights\": weights, \"bin_centres\": distances_c, \"indexes\": idxs_c}\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        # After each epoch except the last, we update the distribution used for fine sampling to improve \n",
    "        # its representativity of the scene's areas of interest. We don't do this after the final epoch\n",
    "        # because the model needs to perform inference using a point distribution it's been trained on.\n",
    "\n",
    "        # Resample new points along the rays using the coarse uniform distribution\n",
    "        self.trainset_c.update_ray_samples(True, None, None)\n",
    "\n",
    "        if self.hparams.use_fine_samples:\n",
    "            weights = torch.concat([out[\"weights\"].cpu() for out in outputs], dim=0)\n",
    "            centres = torch.concat([out[\"bin_centres\"].cpu() for out in outputs], dim=0)\n",
    "            indices = torch.concat([out[\"indexes\"].cpu() for out in outputs], dim=0)\n",
    "            # Put the rays in the same order as in the unshuffled dataset and add a slice of \n",
    "            # zeros to ensure size compatibility (this slice corresponds to the test image)\n",
    "            pix_per_img = self.coarse_data_provider.h * self.coarse_data_provider.w\n",
    "            self.weights = torch.concat(\n",
    "                [\n",
    "                    weights[indices],\n",
    "                    torch.zeros((pix_per_img, weights.size(-2), weights.size(-1)))\n",
    "                ], \n",
    "                dim=0\n",
    "            )\n",
    "            self.centres = torch.concat(\n",
    "                [\n",
    "                    centres[indices], \n",
    "                    centres[-pix_per_img :] + 0.1\n",
    "                ], \n",
    "                dim=0\n",
    "            )\n",
    "\n",
    "            # Create the new fine distribution using the weights calculated this epoch\n",
    "            # self.distribution = PiecewiseConstant(\n",
    "            #     self.weights / self.weights.sum(dim=-2, keepdims=True),\n",
    "            #     self.centres,\n",
    "            #     self.hparams.far_bound,\n",
    "            #     validate_args=False\n",
    "            # )\n",
    "            # Resample new points along the rays using the fine piecewise constant distribution\n",
    "            self.trainset_f.update_ray_samples(True, #self.distribution, \n",
    "                                               None, self.hparams.fine_sample_multiplicator)\n",
    "                \n",
    "    # def on_predict_start(self):\n",
    "    #     # Sample non-stochastic points along the rays (bin centres)\n",
    "    #     self.testset_c.update_ray_samples(False, None, None)\n",
    "            \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx):\n",
    "        self.testset_c.update_ray_samples(False, None, None)\n",
    "        \n",
    "        if dataloader_idx == 0:\n",
    "            points_c, distances_c, _, idx = batch\n",
    "            points_c.squeeze_(0)\n",
    "            distances_c.squeeze_(0)\n",
    "            density_c, rgb_c = self.coarse_model(points_c)\n",
    "            img_cols, img_weights = render_image_colours(distances_c, density_c, rgb_c)\n",
    "\n",
    "            if self.hparams.use_fine_samples:\n",
    "                # Luckily we only execute this section once!\n",
    "                self.weights[-img_weights.size(0) :] = img_weights  # replace placeholder zeros with image's weights\n",
    "                self.centres[-distances_c.size(0) :] = distances_c  # replace placeholder zeros with image's bin centres\n",
    "                # Create the new fine distribution using the weights calculated this epoch\n",
    "                # self.distribution = PiecewiseConstant(\n",
    "                #     self.weights / self.weights.sum(dim=-2, keepdims=True),\n",
    "                #     self.centres,\n",
    "                #     self.hparams.far_bound,\n",
    "                #     validate_args=False\n",
    "                # )\n",
    "                # Resample new samples along the rays using the fine piecewise constant distribution\n",
    "                self.testset_f.update_ray_samples(\n",
    "                    False,\n",
    "                    None, # self.distribution, \n",
    "                    self.hparams.fine_sample_multiplicator\n",
    "                )\n",
    "        \n",
    "        if dataloader_idx == 1:\n",
    "            points_f, distances_f, _, _ = batch    \n",
    "            print(points_f.shape)\n",
    "            points_f.squeeze_(0)\n",
    "            distances_f.squeeze_(0)\n",
    "            density_f, rgb_f = self.fine_model(points_f)\n",
    "            img_cols, _ = render_image_colours(distances_f, density_f, rgb_f)\n",
    "            \n",
    "        return img_cols\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.coarse_data_provider = RayDataProvider(\n",
    "            np_data_filepath=self.hparams.np_data_filepath,\n",
    "            n_samples_per_ray=self.hparams.n_samples_per_ray,\n",
    "            near_bound=self.hparams.near_bound,\n",
    "            far_bound=self.hparams.far_bound,\n",
    "            initial_distribution=None,\n",
    "            fine_sample_multiplicator=None,\n",
    "            test_img_idx=None,\n",
    "            random_state=self.hparams.random_state,\n",
    "        )\n",
    "        self.trainset_c = RayDataset(self.coarse_data_provider, train=True)\n",
    "        self.testset_c = RayDataset(self.coarse_data_provider, train=False)\n",
    "        \n",
    "        if self.hparams.use_fine_samples:\n",
    "            self.fine_data_provider = RayDataProvider(\n",
    "                np_data_filepath=self.hparams.np_data_filepath,\n",
    "                n_samples_per_ray=self.hparams.n_samples_per_ray,\n",
    "                near_bound=self.hparams.near_bound,\n",
    "                far_bound=self.hparams.far_bound,\n",
    "                initial_distribution=None,\n",
    "                fine_sample_multiplicator=self.hparams.fine_sample_multiplicator,\n",
    "                test_img_idx=self.hparams.test_img_idx,\n",
    "                random_state=self.hparams.random_state,\n",
    "            )\n",
    "            self.trainset_f = RayDataset(self.fine_data_provider, train=True)\n",
    "            self.testset_f = RayDataset(self.fine_data_provider, train=False)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        if self.hparams.use_fine_samples:\n",
    "            return {\n",
    "                'coarse': DataLoader(\n",
    "                    self.trainset_c, \n",
    "                    batch_size=self.hparams.batch_size, \n",
    "                    shuffle=True, \n",
    "                    num_workers=self.hparams.num_workers,\n",
    "                ),\n",
    "                'fine': DataLoader(\n",
    "                    self.trainset_f, \n",
    "                    batch_size=self.hparams.batch_size, \n",
    "                    shuffle=True,\n",
    "                    num_workers=self.hparams.num_workers,\n",
    "                )\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'coarse': DataLoader(\n",
    "                    self.trainset_c, \n",
    "                    batch_size=self.hparams.batch_size, \n",
    "                    shuffle=True,\n",
    "                    num_workers=self.hparams.num_workers,\n",
    "                )\n",
    "            }\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        if self.hparams.use_fine_samples:\n",
    "            return [\n",
    "                DataLoader(self.testset_c, batch_size=1, shuffle=False),\n",
    "                DataLoader(self.testset_f, batch_size=1, shuffle=False),\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                DataLoader(self.testset_c, batch_size=1, shuffle=False),\n",
    "            ]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=0.1**(1/self.trainer.max_epochs)\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61102ff-7631-4364-9148-9445b77c7e83",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a44036-c87f-4a3b-8bcf-cb6943304ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b870f7c-6d35-4821-8942-79ee686b4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(DATA_CONFIG['random_state'])\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "callbacks = [\n",
    "    pl.callbacks.EarlyStopping(\n",
    "        monitor=\"train_loss\", \n",
    "        min_delta=RUN_CONFIG[\"early_stop_delta\"],\n",
    "        patience=RUN_CONFIG[\"early_stop_patience\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "if RUN_CONFIG['logging']:\n",
    "    logger = pl.loggers.WandbLogger(\n",
    "        name=RUN_CONFIG[\"run_name\"], project=\"NeRF\", log_model=True, notes=RUN_CONFIG[\"run_notes\"],\n",
    "    )\n",
    "    callbacks.append(pl.callbacks.LearningRateMonitor())\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=callbacks,\n",
    "    reload_dataloaders_every_n_epochs=1,\n",
    "    precision=16 if DATA_CONFIG[\"precision\"] == torch.float16 else 32,\n",
    "    max_epochs=RUN_CONFIG[\"max_epochs\"],\n",
    "    gradient_clip_val=RUN_CONFIG[\"gradient_clip_val\"],\n",
    "    logger=logger if RUN_CONFIG[\"logging\"] else None,\n",
    "    log_every_n_steps=RUN_CONFIG[\"log_every_n_steps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cf46e4-7818-4815-bd69-fa01783c8317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SimpleNeRFModule(**RUN_CONFIG, **MODULE_CONFIG, **DATA_CONFIG)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36358f15-dc46-4537-8bfa-1c518b08a56e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb38d3a-d57d-4898-bd0b-28adc4f9a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# model = SimpleNeRFModule.load_from_checkpoint('D:/experiments/NeRF/NeRF/jewbvzx7/checkpoints/epoch=4-step=1285-v7.ckpt')\n",
    "# model.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95d0e7-1911-410d-b898-2beb57859746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_cols_c, img_cols_f = trainer.predict(model)\n",
    "f = plt.figure(figsize=(12, 6))\n",
    "img_cols_c, img_cols_f = img_cols_c[0], img_cols_f[0]\n",
    "\n",
    "# Normalise the images\n",
    "img_cols_c = (img_cols_c - img_cols_c.amin(keepdim=True)) / (img_cols_c.amax(keepdim=True) - img_cols_c.amin(keepdim=True))\n",
    "img_cols_f = (img_cols_f - img_cols_f.amin(keepdim=True)) / (img_cols_f.amax(keepdim=True) - img_cols_f.amin(keepdim=True))\n",
    "\n",
    "ax1 = f.add_subplot(121)\n",
    "ax1.imshow(img_cols_c.detach().reshape(100, 100, 3).numpy())\n",
    "ax1.set_title(\"Coarse Sampling\")\n",
    "ax2 = f.add_subplot(122)\n",
    "ax2.set_title(\"Fine Sampling\")\n",
    "ax2.imshow(img_cols_f.detach().reshape(100, 100, 3).numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8da15-ce8e-41f2-9f1e-dfa71e00cdcd",
   "metadata": {},
   "source": [
    "The model produces a recognisable image but the details are blurry. With more training epochs and a higher sampling density, this could probably be improved, but it then becomes difficult to train on a consumer-grade GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
